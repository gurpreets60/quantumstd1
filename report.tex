\documentclass[11pt,a4paper]{article}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage{enumitem}
\usepackage{listings}

\lstset{
  basicstyle=\small\ttfamily,
  breaklines=true,
  frame=single,
  language=Python,
  keywordstyle=\color{blue},
  commentstyle=\color{gray},
}

\title{Quantum-Classical Hybrid Stock Prediction\\with Combinatorial Fusion Analysis}
\author{QuantumSTD1 Project Report}
\date{February 2026}

\begin{document}
\maketitle

\begin{abstract}
We present a hybrid quantum-classical stock prediction system that combines a Quantum Long Short-Term Memory (QLSTM) network with seven classical machine learning models via Combinatorial Fusion Analysis (CFA). The QLSTM uses a 5-qubit variational quantum circuit (VQC) with Hadamard, $R_Y$ rotation, and CNOT entangling gates, simulated via PennyLane's \texttt{default.qubit} backend. From an initial pool of 26 classical classifiers trained on the StockNet dataset (87 tickers, 20,315 training / 2,555 validation / 3,720 test samples), a 26-hour progressive training session with CFA greedy forward selection identified 7 winning sklearn models. Combined with the Quantum LSTM, these 8 models form the final ensemble. The system employs a two-phase scaling methodology: rapid prototyping under auto-calibrated time budgets, followed by uncapped full-dataset training for production runs. CFA greedy selection on the 8-model quantum-classical ensemble achieves 54.6\% validation accuracy and 52.0\% test accuracy via average score combination (ASC), with the Quantum LSTM contributing unique cognitive diversity that complements the classical models' decision boundaries.
\end{abstract}

\section{Introduction}

Stock movement prediction is a challenging binary classification task. Given historical price data for a stock over $T$ consecutive trading days, the goal is to predict whether the stock's closing price will \emph{rise or fall} on day $T{+}1$. Since stock movements are influenced by countless factors---macroeconomic conditions, company fundamentals, market sentiment, and stochastic noise---even well-informed models struggle to substantially exceed the 50\% accuracy of a random coin flip. In this regime, a model achieving 53--57\% accuracy represents a meaningful signal: applied systematically across hundreds of trades, even a few percentage points above 50\% can translate to significant cumulative returns.

This work applies \textbf{Combinatorial Fusion Analysis (CFA)}---a framework for combining multiple scoring systems through rank and score functions---to build ensembles of quantum and classical machine learning models. CFA provides six fusion methods:
\begin{itemize}
    \item \textbf{Score-based}: Average Score Combination (ASC), Weighted Score by Cognitive Diversity Strength (WSCDS), Weighted Score by Classifier Performance (WSCP)
    \item \textbf{Rank-based}: Average Rank Combination (ARC), Weighted Rank by Cognitive Diversity Strength (WRCDS), Weighted Rank by Classifier Performance (WRCP)
\end{itemize}

The key insight of CFA is that \emph{diversity among models}---measured through cognitive diversity of their rank-score functions---is more important than individual model accuracy for building effective ensembles.

\section{Experimental Setup}

\subsection{Dataset}

We use the StockNet dataset, a benchmark for stock movement prediction comprising 87 stock tickers drawn from major U.S.\ equities across 652 trading dates. The data is split chronologically:
\begin{itemize}
    \item Training: 20,315 samples (2014-01-02 to 2015-08-02)
    \item Validation: 2,555 samples (2015-08-03 to 2015-09-30)
    \item Test: 3,720 samples (2015-10-01 to end)
\end{itemize}

\textbf{Feature representation.} Each sample consists of 5 consecutive trading days for a single ticker. The raw data provides 7 columns per day: Date, Open, High, Low, Close, Adjusted Close, and Volume. These are preprocessed into 11 Z-score normalized features per time step, derived from OHLCV (Open-High-Low-Close-Volume) statistics. Z-score normalization ($z = (x - \mu) / \sigma$) is computed per-feature across the training period, ensuring zero mean and unit variance. This normalization is critical: raw price values vary by orders of magnitude across tickers (\$10 vs.\ \$700 stocks), so normalization places all tickers in a comparable feature space. The resulting input tensor for each sample has shape $(5, 11)$---five time steps, eleven features each.

\textbf{Labels.} The ground truth is the binary movement direction on day $T{+}1$: 1 if the closing price rises, 0 if it falls. The original dataset encodes labels as $\{-1, 0, +1\}$ (down, neutral, up); we normalize to $[0,1]$ via $(y + 1) / 2$ and binarize at the 0.5 threshold. The dataset is approximately balanced (close to 50/50 up/down), making accuracy a natural evaluation metric.

\textbf{What accuracy means.} A model predicting ``always up'' or ``always down'' achieves roughly 50\% accuracy. Thus, the 50\% level is the effective \emph{random baseline}---not a sign of failure, but the floor below which a model adds no value. In stock prediction, the accuracy gap above 50\% is the meaningful signal. For context, the original StockNet paper reports 58.2\% on this dataset using a deep attention network with supplementary social media features, while models using price data alone typically achieve 51--55\%.

\subsection{The 8 Selected Models}

From an initial pool of 26 classical sklearn classifiers (Table~\ref{tab:sklearn_models}), a 26-hour progressive training session with CFA greedy forward selection identified 7 sklearn models that, when combined, maximize ensemble diversity and accuracy. These 7 models, together with the Quantum LSTM, form the \textbf{8-model final ensemble}. Table~\ref{tab:selected_models} describes each model and explains why it was selected.

\begin{table}[H]
    \centering
    \caption{The 8 Selected Models: Architecture, Decision Mechanism, and Role in the Ensemble}
    \label{tab:selected_models}
    \small
    \begin{tabular}{p{2.8cm}p{2.5cm}p{7.5cm}}
        \toprule
        Model & Family & How It Differs \\
        \midrule
        \textbf{SGD ModHuber} & Linear/SGD & Stochastic gradient descent with modified Huber loss---a smooth approximation to hinge loss that outputs calibrated probabilities. Robust to outliers due to the Huber transition between squared and linear loss. Best individual test accuracy (54.7\%). \\
        \addlinespace
        \textbf{Passive Aggressive} & Linear/Online & An online linear classifier that makes aggressive updates only when a sample violates the current margin. Unlike SGD, it uses a fixed aggressiveness parameter $C{=}0.5$ rather than a learning rate, producing a fundamentally different weight trajectory. \\
        \addlinespace
        \textbf{QDA} & Probabilistic & Quadratic Discriminant Analysis models each class with a separate Gaussian distribution (distinct mean and covariance per class). Unlike linear models, QDA captures \emph{quadratic} decision boundaries, making it sensitive to different feature correlations in up vs.\ down markets. \\
        \addlinespace
        \textbf{Gradient Boost} & Ensemble/Boosting & Builds 20 sequential decision trees (max depth 3), where each tree corrects the residual errors of the ensemble so far. The boosting strategy creates a nonlinear, additive model whose decision surface differs qualitatively from single trees or bagging methods. \\
        \addlinespace
        \textbf{Extra Trees} & Ensemble/Bagging & An ensemble of 10 randomized decision trees (max depth 5) that split on \emph{random} thresholds rather than optimal ones. This extreme randomization produces higher variance per tree but lower correlation between trees, yielding a smoother, more diverse scoring surface than Random Forest. \\
        \addlinespace
        \textbf{Decision Tree} & Tree & A single shallow tree (max depth 3, min leaf size 10) that partitions the feature space into axis-aligned rectangular regions. Its piecewise-constant decision surface is maximally different from smooth linear or probabilistic models, providing high cognitive diversity. \\
        \addlinespace
        \textbf{Bagging DT2} & Ensemble/Bagging & Bags 10 very shallow decision trees (max depth 2), each trained on a 70\% bootstrap subsample. The depth-2 constraint forces each base learner to use at most 3 features, creating a coarse-grained ensemble that captures broad trends rather than fine-grained patterns. \\
        \addlinespace
        \textbf{Quantum LSTM} & Quantum/RNN & A recurrent network where the 4 LSTM gates are implemented as variational quantum circuits (5 qubits, depth 2). The quantum circuit's parameterized rotations and entanglement gates create decision boundaries in a 32-dimensional Hilbert space---a representation fundamentally inaccessible to classical linear algebra. \\
        \bottomrule
    \end{tabular}
\end{table}

\textbf{Why these 8 models?} The selection spans five distinct model families: linear (SGD ModHuber, Passive Aggressive), probabilistic (QDA), tree/ensemble (Gradient Boost, Extra Trees, Decision Tree, Bagging DT2), and quantum (QLSTM). CFA's greedy forward selection showed that this combination maximizes cognitive diversity---each model scores and ranks samples in a fundamentally different way, providing complementary information for fusion. Models from the \emph{same} family but with \emph{different} configurations (e.g., Gradient Boost vs.\ Extra Trees, or Bagging DT2 vs.\ Decision Tree) contribute because their different construction methods (boosting vs.\ bagging, deep vs.\ shallow, random splits vs.\ optimal splits) produce distinct rank-score characteristic curves.

\subsection{Quantum LSTM Architecture}

\subsubsection{Variational Quantum Circuit (VQC)}

The core quantum component is a parameterized variational quantum circuit implemented in PennyLane with a PyTorch interface. The circuit operates on $n_q = n_{\text{input}} + n_{\text{hidden}}$ qubits, where $n_{\text{input}}$ is the compressed feature dimension and $n_{\text{hidden}}$ is the QLSTM hidden state size.

\textbf{Gate sequence per VQC forward pass:}
\begin{enumerate}
    \item \textbf{Hadamard layer}: Apply $H$ to all $n_q$ qubits, creating uniform superposition over the $2^{n_q}$-dimensional computational basis.
    \item \textbf{Data encoding layer}: Apply $R_Y(\theta_i)$ rotation to qubit $i$, encoding the concatenated input--hidden vector $\mathbf{x} \in \mathbb{R}^{n_q}$:
    \[
        R_Y(\theta) = \begin{pmatrix} \cos\frac{\theta}{2} & -\sin\frac{\theta}{2} \\ \sin\frac{\theta}{2} & \cos\frac{\theta}{2} \end{pmatrix}
    \]
    \item \textbf{Variational layers} (repeated $d$ times for VQC depth $d$):
    \begin{enumerate}[label=(\alph*)]
        \item \textbf{Entangling layer}: CNOT gates in a staggered pattern---first on even pairs $(0{\to}1, 2{\to}3, \ldots)$, then odd pairs $(1{\to}2, 3{\to}4, \ldots)$---creating full nearest-neighbor entanglement across the qubit register.
        \item \textbf{Trainable $R_Y$ layer}: Apply $R_Y(w_{k,i})$ with learnable weights $\mathbf{W} \in \mathbb{R}^{d \times n_q}$, initialized as $\mathcal{N}(0, 0.01)$.
    \end{enumerate}
    \item \textbf{Measurement}: Pauli-$Z$ expectation values $\langle \sigma_Z^{(j)} \rangle$ on the first $n_{\text{hidden}}$ qubits, yielding outputs in $[-1, 1]$.
\end{enumerate}

\textbf{Production configuration} (full-dataset runs):
\begin{itemize}
    \item $n_{\text{input}} = 3$ (compressed from 11 raw features)
    \item $n_{\text{hidden}} = 2$
    \item Total qubits: $n_q = 5$
    \item VQC depth: $d = 2$ (two variational layers)
    \item Trainable VQC parameters per gate: $d \times n_q = 10$
    \item Simulator: PennyLane \texttt{default.qubit} (statevector simulation)
\end{itemize}

\subsubsection{QLSTM Cell}

The Quantum LSTM cell replaces the classical linear transformations in a standard LSTM with four independent VQC instances, one for each gate:

\begin{align}
    \mathbf{i}_t &= \sigma\!\big(\text{VQC}_{\text{input}}([\mathbf{x}_t, \mathbf{h}_{t-1}])\big) \label{eq:input_gate} \\
    \mathbf{f}_t &= \sigma\!\big(\text{VQC}_{\text{forget}}([\mathbf{x}_t, \mathbf{h}_{t-1}])\big) \label{eq:forget_gate} \\
    \tilde{\mathbf{c}}_t &= \tanh\!\big(\text{VQC}_{\text{cell}}([\mathbf{x}_t, \mathbf{h}_{t-1}])\big) \label{eq:cell_gate} \\
    \mathbf{o}_t &= \sigma\!\big(\text{VQC}_{\text{output}}([\mathbf{x}_t, \mathbf{h}_{t-1}])\big) \label{eq:output_gate} \\
    \mathbf{c}_t &= \mathbf{f}_t \odot \mathbf{c}_{t-1} + \mathbf{i}_t \odot \tilde{\mathbf{c}}_t \\
    \mathbf{h}_t &= \mathbf{o}_t \odot \tanh(\mathbf{c}_t)
\end{align}

Each VQC gate receives the concatenation $[\mathbf{x}_t, \mathbf{h}_{t-1}] \in \mathbb{R}^{n_q}$ and outputs $n_{\text{hidden}}$ expectation values. The classical sigmoid/tanh activations applied to the VQC outputs preserve the standard LSTM gating dynamics. A final classical linear layer maps $\mathbf{h}_t \in \mathbb{R}^{n_{\text{hidden}}}$ to the scalar prediction output.

\textbf{Total trainable parameters:}
\begin{itemize}
    \item 4 VQC gates $\times$ 10 weights each = 40 quantum parameters
    \item Compression layer: $11 \times 3 + 3 = 36$ parameters (linear, $\mathbb{R}^{11} \to \mathbb{R}^3$)
    \item Output layer: $2 \times 1 + 1 = 3$ parameters (linear, $\mathbb{R}^2 \to \mathbb{R}^1$)
    \item \textbf{Total: 79 trainable parameters}
\end{itemize}

\subsubsection{Full Model Data Flow}

For an input batch of shape $(B, 5, 11)$:
\begin{enumerate}
    \item \textbf{Compression}: Classical linear layer maps each time step from 11 to 3 features: $(B, 5, 11) \to (B, 5, 3)$.
    \item \textbf{Sequential processing}: For each of the 5 time steps, the QLSTM cell processes the compressed features through 4 quantum circuits. At each time step, each circuit requires a full statevector simulation of the 5-qubit system.
    \item \textbf{Output}: The hidden state from the final time step is mapped through a linear layer to produce a scalar prediction.
    \item Total quantum circuit evaluations per sample: $4 \text{ gates} \times 5 \text{ steps} = 20$ VQC forward passes.
\end{enumerate}

\subsection{Training Configuration}

\begin{table}[H]
    \centering
    \caption{QLSTM Training Hyperparameters}
    \begin{tabular}{ll}
        \toprule
        Parameter & Value \\
        \midrule
        Optimizer & RMSprop \\
        Learning rate & 0.01 \\
        Loss function & MSE (mean squared error) \\
        Batch size & 256 \\
        Epochs & 5 per cycle \\
        Data type & float64 (double precision) \\
        Checkpointing & Every epoch + every 300 seconds \\
        \bottomrule
    \end{tabular}
    \label{tab:qlstm_config}
\end{table}

\subsection{Full Classical Model Pool}

The initial model pool comprises 26 sklearn classifiers spanning diverse model families. Each model receives the flattened sequence $(B, 5 \times 11) = (B, 55)$ as input. Table~\ref{tab:sklearn_models} lists all models. Models are auto-discovered at runtime: the system scans the \texttt{models/} directory for Python files exporting a \texttt{NAME} string and a \texttt{make\_model()} factory function, enabling new classifiers to be added by simply dropping a file into the directory.

\begin{table}[H]
    \centering
    \caption{Full Classical Model Pool (26 sklearn classifiers). Bold entries denote the 7 models selected for the final ensemble.}
    \label{tab:sklearn_models}
    \small
    \begin{tabular}{lll}
        \toprule
        Model & Family & Key Hyperparameters \\
        \midrule
        \textbf{Gradient Boosting} & Ensemble/Boosting & \texttt{n\_est=20, max\_depth=3} \\
        Hist GBDT & Ensemble/Boosting & \texttt{max\_iter=30, max\_depth=3, lr=0.1} \\
        AdaBoost & Ensemble/Boosting & \texttt{n\_est=25, lr=0.5} \\
        Random Forest & Ensemble/Bagging & \texttt{n\_est=20} \\
        \textbf{Extra Trees} & Ensemble/Bagging & \texttt{n\_est=10, max\_depth=5} \\
        \textbf{Bagging DT2} & Ensemble/Bagging & \texttt{n\_est=10, max\_samp=0.7, base=DT(d=2)} \\
        \textbf{Decision Tree} & Tree & \texttt{max\_depth=3, min\_leaf=10} \\
        \midrule
        \textbf{SGD Mod.\ Huber} & Linear/SGD & \texttt{loss=modified\_huber, $\alpha$=1e-4} \\
        SGD Hinge & Linear/SGD & \texttt{loss=hinge, $\alpha$=1e-4} \\
        SGD Log & Linear/SGD & \texttt{loss=log\_loss, $\alpha$=1e-4} \\
        Logistic Regression & Linear & \texttt{solver=lbfgs, max\_iter=100} \\
        Ridge & Linear & \texttt{$\alpha$=1.0} \\
        Linear SVC & Linear/SVM & \texttt{C=0.5, max\_iter=2000} \\
        Perceptron & Linear & \texttt{max\_iter=200, tol=1e-3} \\
        \textbf{Passive Aggressive} & Linear & \texttt{C=0.5, max\_iter=200} \\
        \midrule
        \textbf{QDA} & Probabilistic & \texttt{reg\_param=0.5} \\
        LDA & Probabilistic & \texttt{solver=svd} \\
        Gaussian NB & Probabilistic & \texttt{var\_smooth=1e-9} \\
        Gaussian NB 1e-7 & Probabilistic & \texttt{var\_smooth=1e-7} \\
        Gaussian NB 1e-8 & Probabilistic & \texttt{var\_smooth=1e-8} \\
        \midrule
        MLP Classifier & Neural Network & \texttt{layers=(32,16), max\_iter=50} \\
        KNN-11-Dist & Instance-based & \texttt{k=11, weights=distance} \\
        KNN-3 & Instance-based & \texttt{k=3, weights=uniform} \\
        Nearest Centroid & Instance-based & \texttt{metric=euclidean} \\
        \midrule
        Dummy Most Freq & Baseline & \texttt{strategy=most\_frequent} \\
        Dummy Stratified & Baseline & \texttt{strategy=stratified} \\
        \bottomrule
    \end{tabular}
\end{table}

\section{Scaling Methodology: From Prototype to Production}

A key design feature of this system is the \textbf{two-phase scaling methodology} that allows rapid prototyping under tight time budgets, then seamlessly scales to full-dataset training without code changes.

\subsection{Phase 1: Auto-Calibrated Rapid Prototyping}

During development and hyperparameter search, each model operates under a configurable \texttt{time\_budget} (default: 1 second for sklearn, 10 seconds for quantum). The system auto-calibrates the training set size to fit within this budget:

\textbf{Sklearn auto-calibration:}
\begin{enumerate}
    \item \textbf{Pilot fit}: Fit the model on 50 samples and measure wall-clock time to compute $t_{\text{sample}}$ (seconds per sample).
    \item \textbf{Budget allocation}: Reserve 70\% of the time budget for \texttt{fit()}, 30\% for \texttt{predict()}.
    \item \textbf{Sample cap}: Compute $N_{\max} = \lfloor 0.7 \cdot T_{\text{budget}} / t_{\text{sample}} \rfloor$. If $N_{\max} < N_{\text{train}}$, randomly subsample the training set to $N_{\max}$ samples (with fixed seed for reproducibility).
    \item All validation and test samples are always used (inference is fast).
\end{enumerate}

\textbf{Quantum auto-calibration:}
\begin{enumerate}
    \item \textbf{Pilot inference}: Run 2 samples through the quantum circuit and measure $t_{\text{sample}}$.
    \item \textbf{Budget allocation}: Split per-epoch budget 50/50 between training and evaluation.
    \item \textbf{Training cap}: $N_{\text{train}} = \lfloor T_{\text{train}} / (3 \cdot t_{\text{sample}}) \rfloor$, where the $3\times$ factor accounts for forward pass, backward pass, and optimizer step.
    \item \textbf{Eval cap}: $N_{\text{eval}} = \lfloor T_{\text{eval}} / (2 \cdot t_{\text{sample}}) \rfloor$, split between val and test. Uses first-$N$ samples (not random) so prediction indices align across models for CFA.
\end{enumerate}

This phase enables the \textbf{26-hour progressive training session} (${\sim}$2,900 runs): a loop that increases time budgets over time (from 1s to 60s+), allowing the system to explore many model combinations quickly and then refine the best ones with more data. From these 2,900 runs, the CFA greedy forward selection identified the 7 winning sklearn models that form the classical component of the final 8-model ensemble.

\subsection{Phase 2: Uncapped Full-Dataset Training}

For production runs, time and memory budgets are set to effectively unlimited values (\texttt{time\_limit=999999}, \texttt{mem\_limit=0}), causing the auto-calibration to use all available data:

\begin{itemize}
    \item \textbf{Sklearn models}: All 20,315 training samples, all 2,555 val / 3,720 test samples. Training completes in 0.0--7.0 seconds per model (total $<$30 seconds for all 26 models).
    \item \textbf{Quantum LSTM}: All 20,315 training samples per epoch. At ${\sim}$0.19 seconds per sample (20 VQC evaluations $\times$ 5-qubit statevector simulation each), one epoch takes ${\sim}$2.5 hours. A full 5-epoch cycle requires ${\sim}$13 hours on CPU.
\end{itemize}

The 7 selected sklearn models and the Quantum LSTM are then run through a unified inference pipeline that produces aligned prediction files for CFA evaluation.

\subsection{Scaling the Quantum Circuit}

The quantum model's capacity can be scaled along three axes:

\begin{table}[H]
    \centering
    \caption{Quantum Circuit Scaling Parameters}
    \begin{tabular}{lccc}
        \toprule
        Parameter & Current & Moderate & Ambitious \\
        \midrule
        Input qubits ($n_{\text{input}}$) & 3 & 4 & 6 \\
        Hidden qubits ($n_{\text{hidden}}$) & 2 & 3 & 4 \\
        Total qubits ($n_q$) & 5 & 7 & 10 \\
        VQC depth ($d$) & 2 & 3 & 4 \\
        Params/gate ($d \times n_q$) & 10 & 21 & 40 \\
        Total quantum params & 40 & 84 & 160 \\
        Hilbert space dim ($2^{n_q}$) & 32 & 128 & 1,024 \\
        \midrule
        Est.\ time/sample & 0.19s & ${\sim}$0.8s & ${\sim}$5s \\
        Est.\ epoch time (20k) & 2.5h & ${\sim}$4.5h & ${\sim}$28h \\
        \bottomrule
    \end{tabular}
    \label{tab:quantum_scaling}
\end{table}

Statevector simulation scales as $O(2^{n_q})$ in memory and $O(d \cdot 2^{n_q})$ in time per gate application. For $n_q > 10$, GPU-accelerated simulation (e.g., PennyLane's \texttt{lightning.gpu}) or actual quantum hardware would be required.

\textbf{Compression layer scaling:} The classical compression layer ($\mathbb{R}^{11} \to \mathbb{R}^{n_{\text{input}}}$) is the bottleneck for information flow into the quantum circuit. Increasing $n_{\text{input}}$ allows more features to enter the circuit at full resolution rather than being compressed, but requires proportionally more qubits. An alternative is to use a nonlinear compression (e.g., a small MLP) to improve information retention at the same qubit count.

\subsection{Incremental Checkpointing for Long Runs}

Full-dataset quantum training runs exceed 10 hours per cycle. To prevent progress loss, the system implements two levels of checkpointing:
\begin{enumerate}
    \item \textbf{Epoch checkpoints}: Model weights, validation predictions, and test predictions are saved after every epoch completion.
    \item \textbf{Time-based checkpoints}: Model weights are saved every 300 seconds (5 minutes) during the batch training loop within an epoch, ensuring that even if training is interrupted mid-epoch, at most 5 minutes of work is lost.
\end{enumerate}

Predictions are saved in CSV format (\texttt{epoch,prediction,ground\_truth}) aligned by sample index across all models, enabling CFA evaluation on any completed checkpoint without waiting for the full training cycle.

\section{CFA Implementation Details}

\subsection{Core Concepts}

\textbf{Rank-Score Characteristic (RSC):} For a model $A$ with normalized prediction scores $s_A(x_1), \ldots, s_A(x_n)$, the RSC curve is the sorted scores in descending order:
\[
    \text{RSC}_A = \text{sort}(\bar{s}_A, \text{descending})
\]
where $\bar{s}_A$ denotes min-max normalized scores. The RSC captures \emph{how} a model distributes its confidence across samples.

\textbf{Cognitive Diversity (CD):} The diversity between two models $A$ and $B$ is the RMS distance between their RSC curves:
\[
    \text{CD}(A, B) = \sqrt{\frac{1}{n} \sum_{i=1}^{n} \big(\text{RSC}_A(i) - \text{RSC}_B(i)\big)^2}
\]

\textbf{Diversity Strength (DS):} Each model's average diversity against all others:
\[
    \text{DS}(A) = \frac{1}{|M|-1} \sum_{B \in M \setminus \{A\}} \text{CD}(A, B)
\]

\textbf{Train-based normalization:} Scores are min-max normalized using bounds computed from the \emph{validation} predictions, then applied to test predictions with the same bounds. This prevents data leakage from test score distributions.

\subsection{Six Fusion Methods}

Given a subset of models $S \subseteq M$:

\begin{enumerate}
    \item \textbf{ASC} (Average Score Combination): $\hat{y}(x) = \frac{1}{|S|} \sum_{A \in S} \bar{s}_A(x)$
    \item \textbf{ARC} (Average Rank Combination): $\hat{r}(x) = \frac{1}{|S|} \sum_{A \in S} r_A(x)$, where $r_A(x)$ is sample $x$'s rank under model $A$
    \item \textbf{WSCDS}: $\hat{y}(x) = \sum_{A \in S} \frac{\text{DS}(A)}{\sum_{B \in S} \text{DS}(B)} \cdot \bar{s}_A(x)$
    \item \textbf{WRCDS}: $\hat{r}(x) = \sum_{A \in S} \frac{\text{DS}(A)}{\sum_{B \in S} \text{DS}(B)} \cdot r_A(x)$
    \item \textbf{WSCP}: $\hat{y}(x) = \sum_{A \in S} \frac{\text{perf}(A)}{\sum_{B \in S} \text{perf}(B)} \cdot \bar{s}_A(x)$
    \item \textbf{WRCP}: $\hat{r}(x) = \sum_{A \in S} \frac{\text{perf}(A)}{\sum_{B \in S} \text{perf}(B)} \cdot r_A(x)$
\end{enumerate}

For score-based methods, the fused score is thresholded at 0.5 for binary classification. For rank-based methods, predictions are binarized at the median rank.

\subsection{Greedy Forward Selection}

Exhaustive CFA over all $2^{K}$ subsets of $K$ models is computationally intractable for large pools. We use greedy forward selection:

\begin{enumerate}
    \item Sort models by individual validation accuracy; initialize the ensemble with the top performer.
    \item \textbf{Expansion}: Try adding each remaining model to the current ensemble. Evaluate all 6 CFA methods for each candidate. Keep the method yielding the highest validation accuracy.
    \item \textbf{Accept/reject}: If the best candidate improves validation accuracy, add it permanently.
    \item \textbf{Pruning}: When the remaining pool is $>2\times$ the ensemble size, identify models that never improved any combination and remove the worst performer among them.
    \item Repeat until no model improves the ensemble or the pool is exhausted.
\end{enumerate}

This achieves $O(K \cdot N)$ complexity per round ($K$ pool models, $N$ samples, 6 methods), compared to $O(2^K)$ for exhaustive search.

\section{Results}

\subsection{Individual Model Performance}

Figure~\ref{fig:individual} shows validation and test accuracy for all 26 classical models trained on the full dataset. Top performers include SGD Modified Huber (test=54.7\%), QDA (test=53.1\%), and Hist GBDT (test=52.5\%). Most models cluster near the 50\% baseline, reflecting the inherent difficulty of stock prediction from price data alone.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{report_plots/individual_performance.pdf}
    \caption{Individual model performance on the full dataset (2,555 val / 3,720 test). Most models cluster near 50\%, consistent with the inherent difficulty of stock prediction. SGD Modified Huber achieves the highest test accuracy at 54.7\%.}
    \label{fig:individual}
\end{figure}

\subsection{CFA Rank-Score Analysis of the 8-Model Ensemble}

Figure~\ref{fig:rankscore} displays the Rank-Score Characteristic (RSC) curves for the 8 selected models (7 sklearn + Quantum LSTM), evaluated on 124 aligned test samples. In CFA theory, models with \emph{different} RSC curves provide complementary information---even if they have similar overall accuracy. The Quantum LSTM (highlighted with a thicker line) exhibits a distinctly different RSC profile from the classical models: its VQC-based scoring mechanism distributes confidence across samples in a pattern that no classical model replicates. Models whose RSC curves cross at different points contribute unique ranking orderings that improve ensemble performance through CFA fusion.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{report_plots/rsc_ensemble.pdf}
    \caption{Rank-Score Characteristic (RSC) curves for the 8-model ensemble on 124 aligned test samples. Each curve shows how a model distributes its normalized prediction scores across rank positions. The Quantum LSTM (thicker line) exhibits a structurally distinct RSC profile, reflecting its fundamentally different decision mechanism based on quantum circuit measurements.}
    \label{fig:rankscore}
\end{figure}

\subsection{Cognitive Diversity of the 8-Model Ensemble}

Figure~\ref{fig:diversity} shows the pairwise cognitive diversity matrix for the 8-model ensemble. Cognitive diversity measures how differently two models rank and score the same samples---higher values indicate more complementary models. The Quantum LSTM shows high diversity against several classical models, confirming that the quantum circuit's decision boundary operates in a fundamentally different space from classical linear, tree-based, and probabilistic methods.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{report_plots/diversity_heatmap.pdf}
    \caption{Pairwise cognitive diversity matrix for the 8-model ensemble (7 sklearn + Quantum LSTM) on 124 aligned test samples. Higher values (darker) indicate greater disagreement in rank-score patterns, making model pairs more valuable for CFA fusion.}
    \label{fig:diversity}
\end{figure}

\subsection{Classical-Only CFA Ensembles (Full Dataset)}

Using greedy forward selection with CFA on all 26 sklearn models (evaluated on the full 2,555 val / 3,720 test samples), the best classical-only ensemble is:

\begin{table}[H]
    \centering
    \caption{CFA Greedy Selection Results (Full Dataset, Sklearn Only)}
    \begin{tabular}{llccc}
        \toprule
        Ensemble & Method & Models & Val Acc & Test Acc \\
        \midrule
        Gradient Boost (individual) & --- & 1 & 0.5640 & 0.5220 \\
        \textbf{Gradient Boost + Extra Trees} & \textbf{WRCDS} & \textbf{2} & \textbf{0.5808} & \textbf{0.5258} \\
        \bottomrule
    \end{tabular}
    \label{tab:cfa_full}
\end{table}

Figure~\ref{fig:ensemble} shows performance across key sklearn combinations.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{report_plots/cfa_ensemble_performance.pdf}
    \caption{CFA ensemble performance for key sklearn model combinations. The best method is automatically selected for each combination.}
    \label{fig:ensemble}
\end{figure}

\subsection{Historical Best Results (26-Hour Progressive Run)}
Over the 26-hour progressive training session ($\sim$2,900 runs with varying time budgets and data subsets), the best results achieved were:

\begin{table}[H]
    \centering
    \caption{Best Historical CFA Results (Progressive Training)}
    \begin{tabular}{llccc}
        \toprule
        Ensemble & Method & Models & Val Acc & Test Acc \\
        \midrule
        Best Test & WSCDS & 3 & 0.618 & \textbf{0.567} \\
        \footnotesize{(Perceptron + Passive Aggressive + Bagging DT2)} & & & & \\
        \midrule
        Best Val & WSCDS & 7 & \textbf{0.649} & 0.551 \\
        \footnotesize{(SGD ModHuber + PassAgg + QDA + GradBoost +} & & & & \\
        \footnotesize{Extra Trees + Decision Tree + Bagging DT2)} & & & & \\
        \bottomrule
    \end{tabular}
    \label{tab:cfa_historical}
\end{table}

These historical results were obtained during Phase 1 (auto-calibrated prototyping with data subsets), hence the higher apparent accuracy reflects the benefit of model selection on smaller, potentially less noisy subsets.

\subsection{Quantum-Classical CFA (8-Model Ensemble)}

The Quantum LSTM was trained on the full 20,315-sample training set for multiple epochs, then combined with the 7 winning sklearn models for CFA evaluation. Due to the computational cost of quantum inference (${\sim}$0.28s per sample), predictions were aligned on 124 validation and 124 test samples. Greedy forward selection on this 8-model pool yields:

\begin{table}[H]
    \centering
    \caption{Quantum-Classical CFA Results (8-Model Ensemble, 124 Aligned Samples)}
    \begin{tabular}{llccc}
        \toprule
        Ensemble & Method & Models & Val Acc & Test Acc \\
        \midrule
        Quantum LSTM (individual) & --- & 1 & 0.5315 & 0.4841 \\
        Quantum LSTM + Passive Aggressive & ASC & 2 & 0.5444 & 0.5124 \\
        \textbf{Quantum LSTM + PassAgg + Perceptron} & \textbf{ASC} & \textbf{3} & \textbf{0.5460} & \textbf{0.5204} \\
        \bottomrule
    \end{tabular}
    \label{tab:cfa_quantum}
\end{table}

The greedy selection starts with Quantum LSTM (highest individual validation accuracy on the aligned subset) and progressively adds classical models that increase ensemble diversity. The final 3-model combination achieves 54.6\% validation and 52.0\% test accuracy via average score combination (ASC).

\section{Quantum LSTM Analysis}

\subsection{Training Progress}
The Quantum LSTM was trained on the full 20,315 samples for 4 complete epochs (10.7 hours). Figure~\ref{fig:quantum} shows the training convergence.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{report_plots/quantum_training.pdf}
    \caption{Quantum LSTM training progress over 4 epochs. Loss converges by epoch 2. Test accuracy improves from 50.5\% to 51.6\%. Each epoch processes 20,315 samples through the quantum circuit simulation.}
    \label{fig:quantum}
\end{figure}

\subsection{Computational Cost Analysis}

\begin{table}[H]
    \centering
    \caption{Per-Sample Computational Breakdown}
    \begin{tabular}{lcc}
        \toprule
        Operation & Count per sample & Time \\
        \midrule
        Compression (linear) & 5 time steps & $<$0.001s \\
        VQC forward pass ($2^5 = 32$-dim statevector) & 20 (4 gates $\times$ 5 steps) & $\sim$0.18s \\
        Backpropagation (parameter-shift rule) & 20 circuits $\times$ 2 shifts & $\sim$0.01s \\
        \midrule
        \textbf{Total (training)} & & $\sim$0.19s \\
        \textbf{Total (inference)} & & $\sim$0.28s \\
        \bottomrule
    \end{tabular}
    \label{tab:quantum_cost}
\end{table}

\subsection{Limitations and Future Directions}

\begin{enumerate}
    \item \textbf{Computational cost}: At $\sim$0.19 seconds per sample, each training epoch takes $\sim$2.5 hours on CPU. The full 5-epoch cycle requires $\sim$13 hours. Scaling inference to the full 3,720 test samples would require $\sim$17 minutes, feasible but limiting rapid iteration.
    \item \textbf{Circuit capacity}: With only 5 qubits and depth 2, the VQC operates in a 32-dimensional Hilbert space. The 11-dimensional input features are compressed to 3 dimensions before entering the circuit, creating an information bottleneck.
    \item \textbf{Early convergence}: Loss plateaus at $\sim$0.249 after epoch 2, suggesting the current architecture may be near its capacity limit.
    \item \textbf{Diversity advantage}: Despite modest individual accuracy, the Quantum LSTM's decision boundary is fundamentally different from all classical models---parameterized quantum gates vs.\ linear/tree-based decisions. This high cognitive diversity makes it a valuable CFA ensemble partner, as confirmed by the RSC analysis in Figure~\ref{fig:rankscore}.
    \item \textbf{Scalable architecture}: As shown in Table~\ref{tab:quantum_scaling}, increasing to 7 qubits and depth 3 would expand the Hilbert space from 32 to 128 dimensions and more than double the quantum parameter count, substantially increasing expressivity. The modular VQC design means this requires only changing three command-line flags (\texttt{-qi}, \texttt{-qh}, \texttt{-qd}).
\end{enumerate}

\section{Conclusion}

This work demonstrates that CFA effectively combines quantum and classical models for stock prediction. Key findings:

\begin{itemize}
    \item \textbf{CFA improves over individuals}: The best historical ensemble (WSCDS, 3 models from the progressive run) achieves 56.7\% test accuracy vs.\ 54.7\% for the best individual model---a meaningful improvement in stock prediction where even 1--2\% above the 50\% baseline has economic significance.
    \item \textbf{Diversity matters more than accuracy}: The winning ensemble includes models ranked 21st, 22nd, and 4th individually in the progressive run, confirming CFA's principle that diverse scorers outperform homogeneous strong ones.
    \item \textbf{Quantum-classical fusion works}: The 8-model ensemble incorporating the Quantum LSTM achieves 52.0\% test accuracy via ASC on aligned samples. The Quantum LSTM's structurally distinct RSC profile (Figure~\ref{fig:rankscore}) confirms that quantum circuit-based scoring provides cognitive diversity inaccessible to classical models.
    \item \textbf{Two-phase scaling enables efficient exploration}: The auto-calibration mechanism enables rapid exploration (2,900 runs in 26 hours) during prototyping to identify winning model combinations, then seamlessly scales to full-dataset training for production evaluation, without any code changes.
    \item \textbf{Stock prediction remains hard}: Even the best ensembles achieve $\sim$57\% accuracy on this dataset, reflecting the inherent difficulty of predicting short-term stock movements from price data alone. This is consistent with prior literature on the StockNet benchmark.
\end{itemize}

\end{document}
